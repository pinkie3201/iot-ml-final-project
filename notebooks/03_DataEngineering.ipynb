{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports + Config\n",
    "# =========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TARGET_COL = \"Attack_type\"\n",
    "POSITIVE_CLASS = \"DOS_SYN_Hping\"   # 1 = DOS_SYN_Hping, 0 = everything else\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(title)\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56da8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/RT_IOT2022\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217aa1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Binary Target Check\n",
      "================================================================================\n",
      "is_dos_syn_hping\n",
      "1    94659\n",
      "0    28458\n",
      "Name: count, dtype: int64\n",
      "is_dos_syn_hping\n",
      "1    76.89\n",
      "0    23.11\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Create Binary Label (DOS vs Not)\n",
    "# =========================\n",
    "\n",
    "def add_binary_target(df, target_col=TARGET_COL, positive_class=POSITIVE_CLASS, new_col=\"is_dos_syn_hping\"):\n",
    "    \"\"\"\n",
    "    Adds a binary target:\n",
    "      1 if Attack_type == DOS_SYN_Hping\n",
    "      0 otherwise\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2[new_col] = (df2[target_col] == positive_class).astype(int)\n",
    "    return df2\n",
    "\n",
    "df_fe = add_binary_target(df, new_col=\"is_dos_syn_hping\")\n",
    "print_section(\"Binary Target Check\")\n",
    "print(df_fe[\"is_dos_syn_hping\"].value_counts(dropna=False))\n",
    "print(df_fe[\"is_dos_syn_hping\"].value_counts(normalize=True).round(4) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Columns Dropped\n",
      "================================================================================\n",
      "Dropped: ['Unnamed: 0', 'bwd_URG_flag_count']\n",
      "New shape: (123117, 84)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "#  Drop Non-Signal Columns + Constant Feature\n",
    "# =========================\n",
    "\n",
    "def drop_non_signal_columns(df, cols_to_drop=None):\n",
    "    \"\"\"\n",
    "    Drops columns that are index-like or known constant/non-signal.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    if cols_to_drop:\n",
    "        existing = [c for c in cols_to_drop if c in df2.columns]\n",
    "        df2 = df2.drop(columns=existing)\n",
    "    return df2\n",
    "\n",
    "# Recommended drops based on your EDA\n",
    "cols_to_drop = [\"Unnamed: 0\", \"bwd_URG_flag_count\"]  # constant + index\n",
    "df_fe = drop_non_signal_columns(df_fe, cols_to_drop=cols_to_drop)\n",
    "\n",
    "print_section(\"Columns Dropped\")\n",
    "print(\"Dropped:\", [c for c in cols_to_drop if c in df.columns])\n",
    "print(\"New shape:\", df_fe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e955341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Numeric Integrity Check\n",
      "================================================================================\n",
      "Columns with negatives: None found\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Guardrail - Validate Numeric Integrity (No negatives)\n",
    "# =========================\n",
    "\n",
    "def validate_numeric_integrity(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Checks numeric columns for negative values (generally invalid for these flow metrics).\n",
    "    Returns list of columns containing negatives.\n",
    "    \"\"\"\n",
    "    exclude_cols = set(exclude_cols or [])\n",
    "    num_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    num_cols = [c for c in num_cols if c not in exclude_cols]\n",
    "\n",
    "    neg_cols = []\n",
    "    for c in num_cols:\n",
    "        if (df[c] < 0).any():\n",
    "            neg_cols.append(c)\n",
    "    return neg_cols\n",
    "\n",
    "neg_cols = validate_numeric_integrity(df_fe, exclude_cols=[TARGET_COL])\n",
    "print_section(\"Numeric Integrity Check\")\n",
    "print(\"Columns with negatives:\", neg_cols if neg_cols else \"None found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a28ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Port Handling - Option A (Drop Ports)\n",
      "================================================================================\n",
      "Dropped ports: ['id.orig_p', 'id.resp_p']\n",
      "New shape: (123117, 82)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Port Shortcut Control\n",
    "# =========================\n",
    "\n",
    "def drop_ports(df, port_cols=(\"id.orig_p\", \"id.resp_p\")):\n",
    "    df2 = df.copy()\n",
    "    existing = [c for c in port_cols if c in df2.columns]\n",
    "    return df2.drop(columns=existing), existing\n",
    "\n",
    "def bucket_ports(df, port_cols=(\"id.orig_p\", \"id.resp_p\")):\n",
    "    \"\"\"\n",
    "    Converts raw ports into coarse buckets:\n",
    "      - well_known: 0-1023\n",
    "      - registered: 1024-49151\n",
    "      - ephemeral: 49152-65535\n",
    "    Drops original port columns afterward.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "\n",
    "    def bucket_series(s):\n",
    "        s = s.fillna(0).astype(int)\n",
    "        return pd.cut(\n",
    "            s,\n",
    "            bins=[-1, 1023, 49151, 65535],\n",
    "            labels=[\"well_known\", \"registered\", \"ephemeral\"]\n",
    "        )\n",
    "\n",
    "    created = []\n",
    "    for c in port_cols:\n",
    "        if c in df2.columns:\n",
    "            newc = f\"{c}_bucket\"\n",
    "            df2[newc] = bucket_series(df2[c]).astype(\"category\")\n",
    "            created.append(newc)\n",
    "\n",
    "    dropped = [c for c in port_cols if c in df2.columns]\n",
    "    df2 = df2.drop(columns=dropped)\n",
    "    return df2, created, dropped\n",
    "\n",
    "# ---- Option A: DROP PORTS (Recommended for credibility/generalization)\n",
    "df_fe, dropped_ports = drop_ports(df_fe, port_cols=(\"id.orig_p\", \"id.resp_p\"))\n",
    "print_section(\"Port Handling - Option A (Drop Ports)\")\n",
    "print(\"Dropped ports:\", dropped_ports)\n",
    "print(\"New shape:\", df_fe.shape)\n",
    "\n",
    "# ---- Option B: BUCKET PORTS (Comment Option A and use Option B instead)\n",
    "# df_fe, created_buckets, dropped_ports = bucket_ports(df_fe, port_cols=(\"id.orig_p\", \"id.resp_p\"))\n",
    "# print_section(\"Port Handling - Option B (Bucket Ports)\")\n",
    "# print(\"Created:\", created_buckets)\n",
    "# print(\"Dropped:\", dropped_ports)\n",
    "# print(\"New shape:\", df_fe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3ec45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Categorical Columns Types\n",
      "================================================================================\n",
      "proto      category\n",
      "service    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Encode Categorical Features (proto, service)\n",
    "# =========================\n",
    "\n",
    "def encode_categoricals(df, categorical_cols=(\"proto\", \"service\")):\n",
    "    \"\"\"\n",
    "    Converts categorical columns to category dtype (lightweight).\n",
    "    One-hot encoding will be done later before model training.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    for c in categorical_cols:\n",
    "        if c in df2.columns:\n",
    "            df2[c] = df2[c].astype(\"category\")\n",
    "    return df2\n",
    "\n",
    "df_fe = encode_categoricals(df_fe, categorical_cols=(\"proto\", \"service\"))\n",
    "print_section(\"Categorical Columns Types\")\n",
    "print(df_fe[[\"proto\", \"service\"]].dtypes if all(c in df_fe.columns for c in [\"proto\",\"service\"]) else \"proto/service not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd54e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Derived Feature Columns Added\n",
      "================================================================================\n",
      "Added: ['total_pkts', 'total_payload', 'payload_per_pkt', 'syn_to_ack_ratio', 'rst_present', 'is_zero_duration']\n",
      "New shape: (123117, 88)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "#  Add Derived Features Focused on SYN Flood Behavior\n",
    "# =========================\n",
    "\n",
    "def add_dos_derived_features(df):\n",
    "    df2 = df.copy()\n",
    "    eps = 1e-9\n",
    "\n",
    "    # Total packet and payload (you already validated this is informative)\n",
    "    if \"fwd_pkts_tot\" in df2.columns and \"bwd_pkts_tot\" in df2.columns:\n",
    "        df2[\"total_pkts\"] = df2[\"fwd_pkts_tot\"] + df2[\"bwd_pkts_tot\"]\n",
    "\n",
    "    if \"fwd_pkts_payload.tot\" in df2.columns and \"bwd_pkts_payload.tot\" in df2.columns:\n",
    "        df2[\"total_payload\"] = df2[\"fwd_pkts_payload.tot\"] + df2[\"bwd_pkts_payload.tot\"]\n",
    "\n",
    "    if \"total_payload\" in df2.columns and \"total_pkts\" in df2.columns:\n",
    "        df2[\"payload_per_pkt\"] = df2[\"total_payload\"] / (df2[\"total_pkts\"] + eps)\n",
    "\n",
    "    # SYN/ACK dynamics (behavior-first, not port-first)\n",
    "    if \"flow_SYN_flag_count\" in df2.columns and \"flow_ACK_flag_count\" in df2.columns:\n",
    "        df2[\"syn_to_ack_ratio\"] = (df2[\"flow_SYN_flag_count\"] + eps) / (df2[\"flow_ACK_flag_count\"] + eps)\n",
    "\n",
    "    # RST presence can be meaningful in scan/flood traffic\n",
    "    if \"flow_RST_flag_count\" in df2.columns:\n",
    "        df2[\"rst_present\"] = (df2[\"flow_RST_flag_count\"] > 0).astype(int)\n",
    "\n",
    "    # Duration stability guard\n",
    "    if \"flow_duration\" in df2.columns:\n",
    "        df2[\"is_zero_duration\"] = (df2[\"flow_duration\"] <= 0).astype(int)\n",
    "\n",
    "    return df2\n",
    "\n",
    "df_fe = add_dos_derived_features(df_fe)\n",
    "print_section(\"Derived Feature Columns Added\")\n",
    "added = [c for c in [\"total_pkts\",\"total_payload\",\"payload_per_pkt\",\"syn_to_ack_ratio\",\"rst_present\",\"is_zero_duration\"] if c in df_fe.columns]\n",
    "print(\"Added:\", added)\n",
    "print(\"New shape:\", df_fe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964bd342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Clipping Thresholds (p99.9)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flow_iat.tot                1.899466e+08\n",
       "idle.tot                    1.768227e+08\n",
       "payload_bytes_per_second    1.258291e+08\n",
       "idle.max                    5.991468e+07\n",
       "flow_iat.max                5.991468e+07\n",
       "idle.avg                    5.991367e+07\n",
       "flow_iat.std                1.657125e+07\n",
       "idle.std                    1.320453e+07\n",
       "flow_pkts_per_sec           2.097152e+06\n",
       "total_payload               6.737408e+04\n",
       "bwd_pkts_payload.tot        4.156350e+04\n",
       "fwd_pkts_payload.tot        1.063456e+04\n",
       "flow_duration               1.899465e+02\n",
       "total_pkts                  1.673040e+02\n",
       "bwd_pkts_tot                8.288400e+01\n",
       "fwd_pkts_tot                7.400000e+01\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Log Transform Applied\n",
      "================================================================================\n",
      "Transformed columns (existing): ['flow_duration', 'flow_pkts_per_sec', 'payload_bytes_per_second', 'fwd_pkts_tot', 'bwd_pkts_tot', 'fwd_pkts_payload.tot', 'bwd_pkts_payload.tot', 'total_pkts', 'total_payload', 'flow_iat.tot', 'flow_iat.max', 'flow_iat.std', 'idle.tot', 'idle.max', 'idle.avg', 'idle.std']\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Handle Heavy Tails (Log1p + Optional Clipping)\n",
    "# =========================\n",
    "\n",
    "def log1p_transform(df, cols):\n",
    "    df2 = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df2.columns:\n",
    "            # Ensure non-negative before log1p\n",
    "            df2[c] = np.log1p(np.clip(df2[c], a_min=0, a_max=None))\n",
    "    return df2\n",
    "\n",
    "def clip_by_quantile(df, cols, q=0.999):\n",
    "    \"\"\"\n",
    "    Clips each column to its q-quantile upper bound (lower bound fixed at 0).\n",
    "    Returns (df_clipped, clip_thresholds_dict)\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    thresholds = {}\n",
    "    for c in cols:\n",
    "        if c in df2.columns:\n",
    "            ub = df2[c].quantile(q)\n",
    "            thresholds[c] = float(ub)\n",
    "            df2[c] = np.clip(df2[c], 0, ub)\n",
    "    return df2, thresholds\n",
    "\n",
    "# Candidate heavy-tail columns (based on your EDA)\n",
    "heavy_cols = [\n",
    "    \"flow_duration\",\n",
    "    \"flow_pkts_per_sec\",\n",
    "    \"payload_bytes_per_second\",\n",
    "    \"fwd_pkts_tot\", \"bwd_pkts_tot\",\n",
    "    \"fwd_pkts_payload.tot\", \"bwd_pkts_payload.tot\",\n",
    "    \"total_pkts\", \"total_payload\",\n",
    "    \"flow_iat.tot\", \"flow_iat.max\", \"flow_iat.std\",\n",
    "    \"idle.tot\", \"idle.max\", \"idle.avg\", \"idle.std\"\n",
    "]\n",
    "\n",
    "# 1) Optional clipping first, for stability\n",
    "df_fe, clip_thresholds = clip_by_quantile(df_fe, cols=heavy_cols, q=0.999)\n",
    "print_section(\"Clipping Thresholds (p99.9)\")\n",
    "display(pd.Series(clip_thresholds).sort_values(ascending=False).head(20))\n",
    "\n",
    "# 2) Log transform after clipping\n",
    "df_fe = log1p_transform(df_fe, cols=heavy_cols)\n",
    "\n",
    "print_section(\"Log Transform Applied\")\n",
    "print(\"Transformed columns (existing):\", [c for c in heavy_cols if c in df_fe.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ad5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Redundancy Pruning\n",
      "================================================================================\n",
      "Dropped: ['fwd_pkts_per_sec', 'bwd_pkts_per_sec']\n",
      "New shape: (123117, 86)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Correlation-Based Pruning\n",
    "# =========================\n",
    "\n",
    "def prune_redundant_features(df, drop_list):\n",
    "    df2 = df.copy()\n",
    "    existing = [c for c in drop_list if c in df2.columns]\n",
    "    df2 = df2.drop(columns=existing)\n",
    "    return df2, existing\n",
    "\n",
    "# Based on your correlation output: keep flow_pkts_per_sec, drop fwd/bwd rates\n",
    "redundant_drop = [\"fwd_pkts_per_sec\", \"bwd_pkts_per_sec\"]  # keep flow_pkts_per_sec\n",
    "df_fe, dropped = prune_redundant_features(df_fe, redundant_drop)\n",
    "\n",
    "print_section(\"Redundancy Pruning\")\n",
    "print(\"Dropped:\", dropped)\n",
    "print(\"New shape:\", df_fe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79174922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Final Model-Ready Dataset\n",
      "================================================================================\n",
      "Shape: (123117, 85)\n",
      "Label distribution:\n",
      "is_dos_syn_hping\n",
      "1    94659\n",
      "0    28458\n",
      "Name: count, dtype: int64\n",
      "Saved: ../data/processed/rt_iot2022_dos_syn_model_ready.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Final Dataset Assembly\n",
    "# =========================\n",
    "\n",
    "def build_model_ready(df, label_col=\"is_dos_syn_hping\", drop_cols=(TARGET_COL,)):\n",
    "    \"\"\"\n",
    "    Prepares final X/y-ready dataframe:\n",
    "      - drops original multi-class label\n",
    "      - keeps binary label\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    existing_drop = [c for c in drop_cols if c in df2.columns]\n",
    "    df2 = df2.drop(columns=existing_drop)\n",
    "    # Ensure label exists\n",
    "    assert label_col in df2.columns, f\"Missing label col: {label_col}\"\n",
    "    return df2\n",
    "\n",
    "df_model_ready = build_model_ready(df_fe, label_col=\"is_dos_syn_hping\", drop_cols=(TARGET_COL,))\n",
    "print_section(\"Final Model-Ready Dataset\")\n",
    "print(\"Shape:\", df_model_ready.shape)\n",
    "print(\"Label distribution:\")\n",
    "print(df_model_ready[\"is_dos_syn_hping\"].value_counts())\n",
    "\n",
    "# Export\n",
    "import os\n",
    "out_path = \"../data/processed/rt_iot2022_dos_syn_model_ready.csv\"\n",
    "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "df_model_ready.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
